{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.171480Z",
     "start_time": "2019-04-03T18:26:24.438426Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.179446Z",
     "start_time": "2019-04-03T18:26:25.172463Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>|\\'')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "PUNC_TAG = re.compile(r'[^a-zA-Z0-9_]')\n",
    "def remove_punctuation(text):\n",
    "    return PUNC_TAG.sub(' ', text)\n",
    "\n",
    "Spaces = re.compile(r'  *')\n",
    "def remove_spaces(text):\n",
    "    return Spaces.sub(' ', text)\n",
    "\n",
    "def remove_junk(string):\n",
    "    return (remove_spaces(remove_punctuation(remove_tags(string)))).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading stop words in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.271253Z",
     "start_time": "2019-04-03T18:26:25.181441Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/Alir3z4/stop-words/blob/master/english.txt\n",
    "file = open('english.txt', 'r')\n",
    "stopwords = []\n",
    "\n",
    "for i in file.readlines():\n",
    "    i = i.split()\n",
    "    stopwords = stopwords + i\n",
    "\n",
    "file.close()\n",
    "# other stopwords collections\n",
    "# https://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Word Stemming and Lemmatization Functions</h5>\n",
    "<a href=\"https://gist.github.com/mmmayo13/07252b4eb27e5495b6032888b38e5333#file-text_data_preprocessing_5-py\" target=\"_blank\">more on this link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.360478Z",
     "start_time": "2019-04-03T18:26:25.276241Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://gist.github.com/mmmayo13/07252b4eb27e5495b6032888b38e5333#file-text_data_preprocessing_5-py\n",
    "def stem_word(word):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()   \n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def lemmatize_verb(word):\n",
    "    \"\"\"Lemmatize verbs\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word, pos='v')\n",
    "\n",
    "def lemm(word):\n",
    "    \"\"\"Lemmatize nouns\"\"\"\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return wordnet_lemmatizer.lemmatize(word)\n",
    "\n",
    "def pot(word):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return porter_stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.444337Z",
     "start_time": "2019-04-03T18:26:25.365462Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_wordlist(filelocation, stopwords, wordmap, index, MAX_ROWS = 200):\n",
    "    \n",
    "    # parse an xml file by name\n",
    "    mydoc = minidom.parse(filelocation)\n",
    "    \n",
    "    # get each row in the file\n",
    "    items = mydoc.getElementsByTagName('row')\n",
    "    count=0\n",
    "    \n",
    "    for item in items:\n",
    "        \n",
    "        # remove unnecessary things\n",
    "        string = remove_junk(item.attributes['Body'].value)\n",
    "        \n",
    "        # not count a paragraph without any value\n",
    "        if len(string) < 2:\n",
    "            continue\n",
    "        \n",
    "        count=count+1\n",
    "        if count==MAX_ROWS:\n",
    "            break\n",
    "            \n",
    "        \n",
    "        # check every single words in the string\n",
    "        words = string.split(\" \")\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            # check whether it's an important word or not\n",
    "            # https://docs.python.org/3/library/stdtypes.html#str.isnumeric\n",
    "            if not word.isnumeric() and len(word) > 1:\n",
    "                # lemmatize the words \n",
    "                word = lemm(lemmatize_verb(word))\n",
    "                \n",
    "                if word not in wordmap and word not in stopwords:\n",
    "                    \n",
    "                    wordmap[word] = index\n",
    "                    index = index + 1\n",
    "\n",
    "    \n",
    "    return wordmap, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.525997Z",
     "start_time": "2019-04-03T18:26:25.451318Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectormapping_train(filelocation, wordmap, MAX_ROWS = 200):\n",
    "\n",
    "    count = 0\n",
    "    mainvec =[]\n",
    "    \n",
    "    testdoc = minidom.parse(filelocation)\n",
    "    testItems = testdoc.getElementsByTagName('row')\n",
    "\n",
    "    for testItem in testItems:\n",
    "        \n",
    "        # initialize the vector\n",
    "        vector = [0]*len(wordmap)\n",
    "        \n",
    "        # okay, remove the junk as before\n",
    "        string = remove_tags(testItem.attributes['Body'].value)\n",
    "        string = remove_punctuation(string)\n",
    "        string = remove_spaces(string)\n",
    "        string = string.lower()\n",
    "        \n",
    "        # not count a paragraph without any value\n",
    "        if len(string) < 2:\n",
    "            continue\n",
    "        \n",
    "        count=count+1\n",
    "        if count==MAX_ROWS:\n",
    "            break\n",
    "\n",
    "        # check every words in the string\n",
    "        words = string.split(\" \")\n",
    "        \n",
    "        # now go through every word\n",
    "        for w in words:\n",
    "            # lemmatize the word\n",
    "            if not w.isnumeric() and len(w) > 1 and w not in stopwords:\n",
    "\n",
    "                w = lemm(lemmatize_verb(w))\n",
    "\n",
    "                # count how many times it's in the string\n",
    "                if w in wordmap.keys():\n",
    "                    vector[wordmap[w]]=vector[wordmap[w]]+1\n",
    "\n",
    "        # append this vector of a single row to the whole vector list\n",
    "        mainvec.append(vector)\n",
    "\n",
    "    return mainvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.618606Z",
     "start_time": "2019-04-03T18:26:25.527997Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectormapping_test(filelocation, wordmap, MAX_ROWS = 200):\n",
    "\n",
    "    count = 0\n",
    "    mainvec = []\n",
    "    notinwordmap = []\n",
    "    extwordmap = []\n",
    "    \n",
    "    testdoc = minidom.parse(filelocation)\n",
    "    testItems = testdoc.getElementsByTagName('row')\n",
    "    \n",
    "    for testItem in testItems:\n",
    "        \n",
    "        # initialize the vector\n",
    "        vector = [0]*len(wordmap)\n",
    "        notinmap = []\n",
    "        extmap = []\n",
    "        \n",
    "        # okay, remove the junk as before\n",
    "        string = remove_tags(testItem.attributes['Body'].value)\n",
    "        string = remove_punctuation(string)\n",
    "        string = remove_spaces(string)\n",
    "        string = string.lower()\n",
    "        \n",
    "        # not count a paragraph without any value\n",
    "        if len(string) < 2:\n",
    "            continue\n",
    "        \n",
    "        count=count+1\n",
    "        if count==MAX_ROWS:\n",
    "            break\n",
    "        \n",
    "        # check every words in the string\n",
    "        words = string.split(\" \")\n",
    "        \n",
    "        # now go through every word\n",
    "        for w in words:\n",
    "            \n",
    "            # lemmatize the word\n",
    "            if not w.isnumeric() and len(w) > 1 and w not in stopwords:\n",
    "                w = lemm(lemmatize_verb(w))\n",
    "                # print(w)\n",
    "            \n",
    "            # count how many times it's in the string\n",
    "                if w in wordmap.keys():\n",
    "                    vector[wordmap[w]]=vector[wordmap[w]]+1\n",
    "                else:\n",
    "                    notinmap.append(w)\n",
    "        \n",
    "        # https://stackoverflow.com/questions/12282232/how-do-i-count-unique-values-inside-a-list\n",
    "        # extmap = list(set(notinmap))\n",
    "        extmap = (np.unique(notinmap, return_counts=True)[0]).tolist()\n",
    "        notinmap = (np.unique(notinmap, return_counts=True)[1]).tolist()\n",
    "\n",
    "        # append this vector of a single row to the whole vector list\n",
    "        mainvec.append(vector)\n",
    "        notinwordmap.append(notinmap)\n",
    "        extwordmap.append(extmap)\n",
    "        \n",
    "        \n",
    "    return mainvec, notinwordmap, extwordmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:25.696930Z",
     "start_time": "2019-04-03T18:26:25.620602Z"
    }
   },
   "outputs": [],
   "source": [
    "# if we need the total wordmap in a list....\n",
    "def features_name(wordmap):\n",
    "\n",
    "    features = [0] * len(wordmap)\n",
    "\n",
    "    for i in range(len(wordmap)):\n",
    "            for w, c in wordmap.items():\n",
    "                if i == c:\n",
    "                    features[i] = w\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:37.168694Z",
     "start_time": "2019-04-03T18:26:25.697839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee', 'Cooking', 'Law', 'Space', 'Windows_Phone', 'Wood_Working']\n",
      "wordmap generated\n",
      "total number of features: 4129\n"
     ]
    }
   ],
   "source": [
    "filenames = open('./Dataset/topics.txt', 'r')\n",
    "names = []\n",
    "\n",
    "for i in filenames.readlines():\n",
    "    names = names + i.split()\n",
    "\n",
    "print(names)\n",
    "\n",
    "wordmap = {}\n",
    "index = 0\n",
    "MAX_ROWS = 80\n",
    "\n",
    "# build the total wordMap of all the files and rows\n",
    "for name in names:\n",
    "    fileloc = './Dataset/Training/' + name + '.xml'\n",
    "    wordmap, index = build_wordlist(fileloc, stopwords, wordmap, index, MAX_ROWS + 1)\n",
    "\n",
    "\n",
    "print('wordmap generated')\n",
    "print('total number of features:', len(wordmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Mapping for Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:47.325569Z",
     "start_time": "2019-04-03T18:26:37.170688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Coffee--Done\n",
      ">> Cooking--Done\n",
      ">> Law--Done\n",
      ">> Space--Done\n",
      ">> Windows_Phone--Done\n",
      ">> Wood_Working--Done\n",
      "total number of features in wordMap: 4129\n",
      "training vectors of total rows: 480\n",
      "\n",
      "Most Frequent Words:\n",
      "coffee 260\n",
      "phone 94\n",
      "water 94\n",
      "law 93\n",
      "cut 84\n",
      "time 80\n",
      "brew 72\n",
      "space 69\n",
      "wood 64\n",
      "roast 61\n"
     ]
    }
   ],
   "source": [
    "total_vector = []\n",
    "\n",
    "# create a list of vectors of all the training datasets\n",
    "for name in names:\n",
    "    fileloc = './Dataset/Training/' + name + '.xml'\n",
    "    print('>>', name, end='--')\n",
    "    total_vector = total_vector + vectormapping_train(fileloc, wordmap, MAX_ROWS + 1)\n",
    "    print('Done')\n",
    "\n",
    "total_vector = np.array(total_vector)\n",
    "\n",
    "print('total number of features in wordMap:', end=' ')\n",
    "print(len(total_vector[0]))\n",
    "print('training vectors of total rows:', end=' ')\n",
    "print(len(total_vector[:,0]))\n",
    "features = features_name(wordmap)\n",
    "print('\\nMost Frequent Words:')\n",
    "for i in np.argsort(-total_vector.sum(axis=0))[:10]:\n",
    "    print(features[i], total_vector.sum(axis=0)[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-DTF vector conversion for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:59.651622Z",
     "start_time": "2019-04-03T18:26:47.327539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "idf_nu = len(total_vector)\n",
    "tf_idf_train = np.array(total_vector, dtype=np.float64)\n",
    "# print(len(total_vector))\n",
    "\n",
    "for i in range(len(total_vector)):\n",
    "    # print(total_vector[i])\n",
    "    # print(i)\n",
    "    \"\"\"\n",
    "    if sum(total_vector[i]) == 0:\n",
    "        print('zero', i)\n",
    "        tf_idf_train[i] = tf_idf_train[i-1]\n",
    "    else:\n",
    "        tf_idf_train[i] = total_vector[i] / sum(total_vector[i])\n",
    "    \"\"\"\n",
    "    tf_idf_train[i] = total_vector[i] / sum(total_vector[i])\n",
    "    for j in range(len(total_vector[i])):\n",
    "        idf_de = 1 + len((total_vector[:,j])[(total_vector[:,j])>0])\n",
    "        idf = math.log(idf_nu/idf_de)\n",
    "        tf_idf_train[i, j] *= idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:59.666557Z",
     "start_time": "2019-04-03T18:26:59.653590Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "def prediction(vec, KNN=5):\n",
    "    \n",
    "    row_indexes = np.argsort(-vec, axis=None)[:KNN]\n",
    "    # print(row_indexes, end=' ->')\n",
    "    predic = []\n",
    "    for r in row_indexes:\n",
    "        val = MAX_ROWS\n",
    "\n",
    "        for i in range(0,len(names)):\n",
    "            if r < val:\n",
    "                predic.append(names[i])\n",
    "                break\n",
    "            else:\n",
    "                val = val + MAX_ROWS\n",
    "\n",
    "    b = np.unique(predic, return_counts=True)[1]\n",
    "    c = np.argmax(b)\n",
    "    for i in range(len(b)):\n",
    "        if i != c and b[i] == b[c]:\n",
    "            return predic[0]\n",
    "    \n",
    "    return np.unique(predic)[np.argmax(np.unique(predic, return_counts=True)[1])]\n",
    "\n",
    "\n",
    "\n",
    "def actual_result_from_id(names, total_numbers, TEST_MAX_ROWS):\n",
    "    val = TEST_MAX_ROWS\n",
    "    for i in range(0,len(names)):\n",
    "        if total_numbers < val:\n",
    "            return names[i]\n",
    "        \n",
    "        else:\n",
    "            val = val + TEST_MAX_ROWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try this on one test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:26:59.798203Z",
     "start_time": "2019-04-03T18:26:59.670544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>> Coffee <<<<<<<<<<<<<<<<<<<\n",
      "coffee 14\n",
      "grind 11\n",
      "bean 7\n",
      "brew 5\n",
      "espresso 5\n",
      "air 4\n",
      "time 4\n",
      "ground 3\n",
      "dose 3\n",
      "process 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "namet = 'Coffee'\n",
    "fileloc = './Dataset/Test/' + namet + '.xml'\n",
    "actual_result = namet\n",
    "print('>>>>>>>>>>>>>>>>>>>', namet, '<<<<<<<<<<<<<<<<<<<')\n",
    "\n",
    "TEST_MAX_ROWS = 5\n",
    "test_vectors = []\n",
    "extvector = []\n",
    "extwordmap = []\n",
    "\n",
    "test_vectors, extvector, extwordmap = vectormapping_test(fileloc, wordmap, TEST_MAX_ROWS + 1)\n",
    "test_vectors = np.array(test_vectors)\n",
    "extvector = np.array(extvector)\n",
    "extwordmap = np.array(extwordmap)\n",
    "\n",
    "for i in np.argsort(-test_vectors.sum(axis=0))[:10]:\n",
    "    print(features[i], test_vectors.sum(axis=0)[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:27:00.037561Z",
     "start_time": "2019-04-03T18:26:59.803206Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# test data\n",
    "idf_nu = len(total_vector)\n",
    "tf_idf_test = np.array(test_vectors, dtype=np.float64)\n",
    "\n",
    "for i in range(len(test_vectors)):\n",
    "    # first tf calculation\n",
    "    tf_idf_test[i] = test_vectors[i] / (sum(test_vectors[i]) + sum(extvector[i]))\n",
    "    \n",
    "    # idf calculation and then tf-idf calculation\n",
    "    for j in range(len(test_vectors[i])):\n",
    "        idf_de = 1 + len((total_vector[:,j])[(total_vector[:,j])>0])\n",
    "        idf = math.log(idf_nu/ idf_de)\n",
    "        tf_idf_test[i, j] *= idf\n",
    "\n",
    "\n",
    "# not in wordlist\n",
    "idf = math.log(idf_nu)\n",
    "tf_idf_ext = np.empty([len(extvector), 1], dtype=np.float64)\n",
    "\n",
    "for i in range(len(extvector)):\n",
    "    tf_idf_ext[i] = sum(((extvector[i] / (sum(test_vectors[i]) + sum(extvector[i]))) * idf) ** 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:27:03.211079Z",
     "start_time": "2019-04-03T18:27:00.042571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN =  5\n",
      "Coffee --> Coffee\n",
      "Coffee --> Coffee\n",
      "Coffee --> Coffee\n",
      "Coffee --> Coffee\n",
      "Coffee --> Coffee\n",
      "accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accurate = 0\n",
    "KNN = 5\n",
    "print('KNN = ', KNN)\n",
    "\n",
    "for i in range(len(tf_idf_test)):\n",
    "    numerator_set = np.multiply(tf_idf_train, tf_idf_test[i])\n",
    "    denB = sum(tf_idf_test[i] ** 2) + tf_idf_ext[i]\n",
    "    denA_set = tf_idf_train ** 2\n",
    "    \n",
    "    resulting_vec = np.empty((len(numerator_set), 1), dtype=float)\n",
    "    for j in range(len(numerator_set)):\n",
    "        resulting_vec[j] = sum(numerator_set[j]) / (math.sqrt(sum(denA_set[j])) * math.sqrt(denB))\n",
    "    \n",
    "    predicted_result = prediction(resulting_vec, KNN)\n",
    "    print(actual_result,\"-->\", predicted_result)\n",
    "    if actual_result == predicted_result:\n",
    "        accurate += 1\n",
    "\n",
    "print('accuracy:', (accurate/TEST_MAX_ROWS) * 100, '%')\n",
    "# https://www.python-course.eu/python3_formatted_output.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>cosine Similarity for all the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:27:06.025557Z",
     "start_time": "2019-04-03T18:27:03.213074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>> Coffee <<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>> Cooking <<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>> Law <<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>> Space <<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>> Windows_Phone <<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>> Wood_Working <<<<<<<<<<<<<<<<<\n",
      "\n",
      "Most Frequent Words:\n",
      "coffee 70\n",
      "brew 43\n",
      "time 38\n",
      "wood 34\n",
      "grind 31\n",
      "water 28\n",
      "temperature 23\n",
      "phone 23\n",
      "power 19\n",
      "launch 18\n"
     ]
    }
   ],
   "source": [
    "TEST_MAX_ROWS = 20\n",
    "test_vectors = []\n",
    "extvector = []\n",
    "extwordmap = []\n",
    "\n",
    "for name in names:\n",
    "    fileloc = './Dataset/Test/' + name + '.xml'\n",
    "    print('>>>>>>>>>>>>>>>>>>>', name, '<<<<<<<<<<<<<<<<<')\n",
    "\n",
    "    test_vectors += vectormapping_test(fileloc, wordmap, TEST_MAX_ROWS + 1)[0]\n",
    "    extvector += vectormapping_test(fileloc, wordmap, TEST_MAX_ROWS + 1)[1]\n",
    "    extwordmap += vectormapping_test(fileloc, wordmap, TEST_MAX_ROWS + 1)[2]\n",
    "\n",
    "test_vectors = np.array(test_vectors)\n",
    "extvector = np.array(extvector)\n",
    "extwordmap = np.array(extwordmap)\n",
    "\n",
    "print('\\nMost Frequent Words:')\n",
    "for i in np.argsort(-test_vectors.sum(axis=0))[:10]:\n",
    "    print(features[i], test_vectors.sum(axis=0)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:27:09.761569Z",
     "start_time": "2019-04-03T18:27:06.027551Z"
    }
   },
   "outputs": [],
   "source": [
    "# test data tf_idf vector creation\n",
    "idf_nu = len(total_vector)\n",
    "tf_idf_test = np.array(test_vectors, dtype=np.float64)\n",
    "\n",
    "for i in range(len(test_vectors)):\n",
    "    tf_idf_test[i] = test_vectors[i] / (sum(test_vectors[i]) + sum(extvector[i]))\n",
    "    \n",
    "    for j in range(len(test_vectors[i])):\n",
    "        idf_de = 1 + len((total_vector[:,j])[(total_vector[:,j])>0])\n",
    "        idf = math.log10(idf_nu/ idf_de)\n",
    "        tf_idf_test[i, j] *= idf\n",
    "\n",
    "\n",
    "# not in wordlist\n",
    "idf = math.log(idf_nu)\n",
    "tf_idf_ext = np.empty([len(extvector), 1], dtype=np.float64)\n",
    "\n",
    "for i in range(len(extvector)):\n",
    "    tf_idf_ext[i] = sum(((extvector[i] / (sum(test_vectors[i]) + sum(extvector[i]))) * idf) ** 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T18:29:55.687040Z",
     "start_time": "2019-04-03T18:27:09.762566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN =  1\n",
      "-----------------------------------------------\n",
      "accuracy: 77.5 %\n",
      "-----------------------------------------------\n",
      "KNN =  3\n",
      "-----------------------------------------------\n",
      "accuracy: 79.167 %\n",
      "-----------------------------------------------\n",
      "KNN =  5\n",
      "-----------------------------------------------\n",
      "accuracy: 81.667 %\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file = open('KNN cosine similarity report.txt', 'w')\n",
    "file.write('KNN \\taccuracy\\n')\n",
    "\n",
    "KNNs = [1, 3, 5]\n",
    "\n",
    "for KNN in KNNs:\n",
    "\n",
    "    accurate = 0\n",
    "    total_numbers = 0\n",
    "    \n",
    "    print('KNN = ', KNN)\n",
    "    file.write(str(KNN))\n",
    "\n",
    "    for i in range(len(tf_idf_test)):\n",
    "        numerator_set = np.multiply(tf_idf_train, tf_idf_test[i])\n",
    "        denB = sum(tf_idf_test[i] ** 2) + tf_idf_ext[i]\n",
    "        denA_set = tf_idf_train ** 2\n",
    "\n",
    "        resulting_vec = np.empty((len(numerator_set), 1), dtype=float)\n",
    "        for j in range(len(numerator_set)):\n",
    "            resulting_vec[j] = sum(numerator_set[j]) / (math.sqrt(sum(denA_set[j])) * math.sqrt(denB))\n",
    "\n",
    "        actual_result = actual_result_from_id(names, total_numbers, TEST_MAX_ROWS)\n",
    "        predicted_result = prediction(resulting_vec, KNN)\n",
    "\n",
    "        total_numbers += 1\n",
    "        # print(actual_result,\"-->\", predicted_result)\n",
    "        if actual_result == predicted_result:\n",
    "            accurate += 1\n",
    "        # else:\n",
    "        #     print(actual_result, predicted_result)\n",
    "    \n",
    "    accuracy = round((accurate/total_numbers) * 100, 3)\n",
    "    print('-----------------------------------------------')\n",
    "    print('accuracy:', accuracy, '%')\n",
    "    print('-----------------------------------------------')\n",
    "    file.write('\\t'+ str(accuracy) + '\\n')\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
